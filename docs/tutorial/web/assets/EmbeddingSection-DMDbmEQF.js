import{u as p,r,j as e}from"./index-DB782DBc.js";import{C as x,S as u}from"./SourcePanel-BwVAQ71f.js";import{M as c}from"./constants-BXPCUJh7.js";import{m as f}from"./utils-ZSIfeTeC.js";const m=[{text:"你",id:340},{text:"好",id:590},{text:"学",id:670},{text:"习",id:680},{text:"AI",id:1100},{text:"。",id:16}];function v(){const d=f(42),n={};return m.forEach(o=>{n[o.id]=Array.from({length:c.hidden_size},()=>(d()-.5)*2)}),n}const g=v();function T(){const{isDark:d}=p(),[n,o]=r.useState(0),l=m[n],_=r.useMemo(()=>{const t=l.id,i=[];for(let s=Math.max(0,t-2);s<=Math.min(c.vocab_size-1,t+2);s++)i.push(s);return i.map(s=>{const a=s===t,h=g[s]||(()=>{const y=f(s*512+7);return Array.from({length:c.hidden_size},()=>(y()-.5)*2)})();return{id:s,isTarget:a,vals:h}})},[l.id]),b=r.useMemo(()=>{const t=g[l.id],i=64,s=Math.max(...t.slice(0,i).map(Math.abs));return t.slice(0,i).map((a,h)=>({value:a,height:Math.abs(a)/s*80,dim:h}))},[l.id]),j=r.useMemo(()=>{const t=d?"#e2e8f0":"#1a1a2e",i=d?"#818cf8":"#4f46e5",s=d?"#34d399":"#10b981";return`
      <rect x="20" y="30" width="150" height="50" rx="8" fill="none" stroke="${i}" stroke-width="2"/>
      <text x="95" y="60" text-anchor="middle" fill="${t}" font-size="14">Embedding</text>
      <text x="95" y="100" text-anchor="middle" fill="${t}" font-size="11">[6400 × 512]</text>
      <rect x="430" y="30" width="150" height="50" rx="8" fill="none" stroke="${i}" stroke-width="2"/>
      <text x="505" y="60" text-anchor="middle" fill="${t}" font-size="14">LM Head</text>
      <text x="505" y="100" text-anchor="middle" fill="${t}" font-size="11">[512 × 6400]</text>
      <line x1="170" y1="55" x2="430" y2="55" stroke="${s}" stroke-width="2" stroke-dasharray="8,4"/>
      <text x="300" y="48" text-anchor="middle" fill="${s}" font-size="12" font-weight="bold">权重共享 (Weight Tying)</text>
      <text x="300" y="140" text-anchor="middle" fill="${t}" font-size="11" opacity="0.7">embed_tokens.weight = lm_head.weight → 节省 512×6400 = 3.3M 参数</text>
    `},[d]);return e.jsxs(e.Fragment,{children:[e.jsx("h2",{children:"2. Token Embedding"}),e.jsxs("p",{className:"desc",children:["把 token ID 映射成一个 ",e.jsx("code",{children:"hidden_size=512"})," 维的向量，相当于 ",e.jsx("code",{children:"nn.Embedding(6400, 512)"})," 的查表操作： 输入 token ID，输出一行 512 维浮点数。MiniMind 中 Embedding 层和 LM Head 共享同一个权重矩阵（",e.jsx("code",{children:"tie_word_embeddings=True"}),"）。",e.jsx("br",{}),e.jsxs("small",{style:{color:"var(--fg2)"},children:["关联源码：",e.jsx("code",{children:"model/model_minimind.py:381"})," ",e.jsx("code",{children:"self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)"})]})]}),e.jsxs(x,{title:"矩阵查找动画",children:[e.jsx("p",{style:{marginBottom:10,fontSize:"0.9rem",color:"var(--fg2)"},children:"Embedding 本质上是一次查表操作：以 token ID 为行索引，从形状为 [6400, 512] 的权重矩阵中取出对应行， 得到一个 512 维的稠密向量。这个向量就是该 token 的语义表示，模型后续所有计算都基于它。 点击下方示例 token，左侧矩阵高亮对应行，右侧柱状图展示前 64 维的数值分布（蓝色=正值，红色=负值）："}),e.jsx("div",{style:{display:"flex",gap:8,flexWrap:"wrap",marginBottom:12},children:m.map((t,i)=>e.jsxs("button",{className:`btn${i===n?" primary":""}`,onClick:()=>o(i),children:['"',t.text,'" (',t.id,")"]},t.id))}),e.jsxs("div",{className:"viz-grid",children:[e.jsxs("div",{children:[e.jsx("div",{className:"label",children:"Embedding 矩阵 (6400 × 512) 局部视图"}),e.jsx("div",{style:{overflow:"auto",maxHeight:260},children:e.jsxs("table",{className:"matrix-table",children:[e.jsx("thead",{children:e.jsxs("tr",{children:[e.jsx("th",{children:"ID"}),Array.from({length:8},(t,i)=>e.jsxs("th",{children:["d",i]},i)),e.jsx("th",{children:"..."}),[508,509,510,511].map(t=>e.jsxs("th",{children:["d",t]},t))]})}),e.jsx("tbody",{children:_.map(t=>e.jsxs("tr",{children:[e.jsx("td",{className:t.isTarget?"highlight":"",children:t.id}),Array.from({length:8},(i,s)=>e.jsx("td",{className:t.isTarget?"highlight":"",children:t.vals[s].toFixed(2)},s)),e.jsx("td",{className:t.isTarget?"highlight":"",children:"..."}),[508,509,510,511].map(i=>e.jsx("td",{className:t.isTarget?"highlight":"",children:t.vals[i].toFixed(2)},i))]},t.id))})]})})]}),e.jsxs("div",{children:[e.jsx("div",{className:"label",children:"输出向量 (512 维) 前 64 维可视化"}),e.jsx("div",{style:{height:180,display:"flex",alignItems:"flex-end",gap:1},children:b.map((t,i)=>e.jsx("div",{style:{flex:1,minWidth:3,height:t.height,background:t.value>=0?"var(--accent)":"var(--red)",borderRadius:"1px 1px 0 0",transition:"height 0.4s ease"},title:`d${t.dim}: ${t.value.toFixed(4)}`},i))})]})]}),e.jsx(u,{title:"对照源码：model/model_minimind.py:381, 434-435",code:`# MiniMindModel.__init__
# 创建 Embedding 查找表：输入 token_id (0~6399) → 输出 512 维向量
self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)  # [6400, 512]

# MiniMindForCausalLM.__init__ — 权重共享 (Weight Tying)
# LM Head 将隐藏状态投影回词表空间 [512] → [6400]
self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)
# 关键：让 embed_tokens 和 lm_head 共享同一个 Parameter
# 这样 embed_tokens.weight 和 lm_head.weight 指向同一块 GPU 显存
# 效果：节省 3.3M 参数 + 输入/输出语义空间对齐
self.model.embed_tokens.weight = self.lm_head.weight  # 共享！`})]}),e.jsxs(x,{title:"权重共享 (Weight Tying)",children:[e.jsx("p",{style:{marginBottom:10,fontSize:"0.9rem",color:"var(--fg2)"},children:"输入端的 Embedding 层（token ID → 向量）和输出端的 LM Head（向量 → logits）共享同一个权重矩阵 [6400, 512]。 这意味着语义相近的 token 在输入空间和输出空间中表现一致，同时节省了 512 × 6400 = 3.3M 参数（约占小模型 12%）。 下图中虚线表示两个矩阵指向同一块内存，训练时梯度会同时更新它们。"}),e.jsx("svg",{width:"100%",height:160,viewBox:"0 0 600 160",dangerouslySetInnerHTML:{__html:j}})]})]})}export{T as default};
