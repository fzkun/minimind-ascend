# 第一章：大语言模型入门 — 从零认识 LLM

> 欢迎来到 MiniMind 教程系列！本章将带你从最直觉的角度理解大语言模型（LLM）的核心原理，
> 并介绍 MiniMind 项目的定位和整体训练流程。不需要任何前置知识，放轻松，我们开始吧。

---

## 1. 什么是大语言模型？

### 一个"超级文字接龙游戏"

还记得小时候玩的文字接龙吗？一个人说"阳光"，下一个人接"光明"，再接"明天"……
每个人根据前面的词，想出一个最合适的词接上去。

**大语言模型（Large Language Model，简称 LLM）本质上就是一个"超级文字接龙选手"。**

不过，它和人类小朋友的接龙有几个关键区别：

```
普通文字接龙：  阳光 → 光明 → 明天 → ...（只看最后一个字）

LLM 的接龙：   给定 "今天天气真" → 预测下一个词 → "好"
               给定 "今天天气真好" → 预测下一个词 → "，"
               给定 "今天天气真好，" → 预测下一个词 → "适合"
               ...一个词一个词地接下去，最终生成完整的句子。
```

它不只看最后一个字，而是**综合考虑前面所有的内容**，然后选出"最可能的下一个词"。
这就是 LLM 的全部秘密——听起来简单，但当模型足够大、读过的文字足够多时，
这个简单的游戏规则就能涌现出令人惊叹的语言能力。

### 类比总结

| 概念 | 类比 |
|------|------|
| 大语言模型 | 读过海量书籍的"超级文字接龙选手" |
| 训练过程 | 让这个选手反复练习，学会"什么词最可能跟在什么词后面" |
| 生成文本 | 一个词一个词地"接龙"，直到生成完整回答 |
| 模型参数 | 选手大脑中的"经验值"——参数越多，记住的模式就越丰富 |

---

## 2. 核心思想："下一个词预测"

LLM 的核心任务可以用一句话概括：

> **给定前面的词，预测下一个最可能出现的词。**

用数学的方式写就是：

```
P(下一个词 | 前面所有的词)
```

举个例子：

```
输入：  "中国的首都是"
模型预测概率分布：
  "北京"  → 92.3%
  "上海"  → 3.1%
  "南京"  → 1.8%
  "天津"  → 0.5%
  ...其他  → 2.3%
```

模型会选择概率最高的"北京"作为输出（或者根据"温度"参数引入一些随机性）。
然后把"北京"拼接到输入后面，继续预测下一个词：

```
输入：  "中国的首都是北京"
模型预测：  "，" → 78.5%  |  "。" → 15.2%  |  ...
```

就这样一个词一个词地生成，直到模型输出结束标记或达到最大长度。
这个过程叫做**自回归生成（Autoregressive Generation）**。

### 训练 = 学会预测

那模型怎么学会这种预测能力的呢？答案是：**给它看大量文本，让它练习预测。**

```
训练数据：  "中国的首都是北京，是一座历史悠久的城市。"

训练时模型的任务：
  输入 "中国"       → 应该预测 "的"
  输入 "中国的"     → 应该预测 "首都"
  输入 "中国的首都" → 应该预测 "是"
  输入 "中国的首都是" → 应该预测 "北京"
  ...以此类推
```

每次预测错了，模型就调整自己的参数（"经验值"），让下次预测得更准。
经过数十亿条文本的反复训练，模型就逐渐学会了语言的规律。

---

## 3. MiniMind 在 LLM 生态中的定位

当前主流的 LLM（如 GPT-4、LLaMA-70B、Qwen-72B 等）动辄拥有数百亿甚至上万亿参数，
训练一次需要成百上千张 GPU 和数周时间。这对于个人学习者来说是不可触及的。

**MiniMind 的目标是：让你在一张消费级显卡上，完整体验 LLM 的全部训练流程。**

### MiniMind 的特点

```
+------------------------------------------------------+
|                  MiniMind 项目定位                     |
+------------------------------------------------------+
|                                                      |
|  - 模型规模：26M ~ 145M 参数（对比 GPT-3 的 175B）     |
|  - 硬件要求：单张 RTX 3090 即可完成全流程训练           |
|  - 技术栈：  纯 PyTorch 实现，不依赖第三方训练框架       |
|  - 定位：    教学导向，代码清晰可读                     |
|  - 覆盖：    从预训练到对齐的完整 pipeline               |
|                                                      |
+------------------------------------------------------+
```

### 模型规格一览

| 配置 | 参数量 | 隐藏层维度 (hidden_size) | 层数 (num_hidden_layers) | 说明 |
|------|--------|--------------------------|--------------------------|------|
| Small | 26M | 512 | 8 | 入门款，训练最快 |
| Base | 104M | 768 | 16 | 标准款，效果更好 |
| MoE | 145M | 768 + MoE | 16 | 混合专家架构，进阶学习 |

这些规模虽然远小于商业模型，但**麻雀虽小、五脏俱全**——
模型架构（Transformer、RoPE、GQA、Flash Attention、MoE）和训练流程
（预训练、SFT、RLHF、DPO、PPO、GRPO 等）都与大模型完全一致。
理解了 MiniMind 的原理，你就理解了 GPT 系列的核心。

---

## 4. 训练全流程鸟瞰图

一个 LLM 从"什么都不懂"到"能和你流畅对话"，需要经历以下几个阶段：

```
                         MiniMind 训练全流程
===========================================================================

阶段一：预训练（Pretrain）           阶段二：监督微调（SFT）
让模型学会"语言"                    让模型学会"对话"

  大量无标注文本                      人工标注的对话数据
       |                                   |
       v                                   v
  +------------+                     +------------+
  |  预训练     |  ---- 权重 ---->   |  全参 SFT   |
  | (Pretrain)  |                     | (Full SFT)  |
  +------------+                     +------------+
                                           |
                          +----------------+----------------+
                          |                                 |
                          v                                 v
                   +------------+                    +------------+
                   |  LoRA 微调  |                    |  知识蒸馏   |
                   |   (LoRA)    |                    |(Distill.)  |
                   +------------+                    +------------+

===========================================================================

阶段三：对齐（Alignment）            阶段四：推理（Inference）
让模型的回答更符合人类偏好             部署与使用

  偏好数据 / 奖励模型                  训练好的模型权重
       |                                   |
       v                                   v
  +----------+  +----------+          +------------+
  |   DPO    |  |   PPO    |          |  推理对话   |
  +----------+  +----------+          | (eval_llm)  |
  +----------+  +----------+          +------------+
  |   GRPO   |  |   SPO    |
  +----------+  +----------+
  +----------+
  | 推理训练  |
  | (Reason) |
  +----------+

===========================================================================
```

用一条线来总结这个流程：

```
预训练 ──> SFT ──> 对齐（DPO/PPO/GRPO/SPO/Reason） ──> 推理部署
  |         |                  |                           |
学会语言  学会对话          学会"做好人"                  上线使用
```

### 每个阶段在做什么？

| 阶段 | 目标 | 通俗解释 |
|------|------|----------|
| 预训练 | 学习语言知识 | 像小孩大量阅读，积累词汇和常识 |
| SFT（监督微调） | 学习对话格式 | 像学生上课，老师教它如何正确回答问题 |
| LoRA 微调 | 轻量级适配 | 只调整少量参数，快速适配新任务（如医疗问答） |
| 知识蒸馏 | 向大模型学习 | 小模型当"学徒"，模仿大模型的输出 |
| DPO / PPO / GRPO / SPO | 偏好对齐 | 让模型学会分辨"好回答"和"坏回答" |
| 推理训练（Reason） | 学会思考 | 让模型在回答前先"想一想"，输出思维链 |

---

## 5. 训练脚本对照表

MiniMind 的每个训练阶段都对应 `trainer/` 目录下的一个独立 Python 脚本。
下表展示了它们的完整对应关系：

| 训练阶段 | 脚本文件 | 数据集类 | 输入数据格式 | 说明 |
|---------|----------|---------|------------|------|
| 预训练 | `trainer/train_pretrain.py` | `PretrainDataset` | 无标注文本 JSONL | 学习语言建模能力 |
| 全参 SFT | `trainer/train_full_sft.py` | `SFTDataset` | 对话格式 JSONL | 学习指令跟随和对话 |
| LoRA 微调 | `trainer/train_lora.py` | `SFTDataset` | 对话格式 JSONL | 低秩适配，轻量微调 |
| DPO | `trainer/train_dpo.py` | `DPODataset` | 偏好对（chosen/rejected） | 直接偏好优化 |
| PPO | `trainer/train_ppo.py` | `RLAIFDataset` | 提示词（在线生成） | 近端策略优化，带 Critic |
| GRPO | `trainer/train_grpo.py` | `RLAIFDataset` | 提示词（在线生成） | 分组相对策略优化 |
| SPO | `trainer/train_spo.py` | `RLAIFDataset` | 提示词（在线生成） | 自适应偏好优化 |
| 知识蒸馏 | `trainer/train_distillation.py` | `SFTDataset` | 对话格式 JSONL | 大模型指导小模型学习 |
| 推理训练 | `trainer/train_reason.py` | `SFTDataset` | 含思维链的对话 JSONL | 学习 `<think>...</think>` 推理格式 |

推理与对话脚本：

| 功能 | 脚本文件 | 说明 |
|-----|---------|------|
| 模型推理 | `eval_llm.py` | 加载训练好的权重，进行交互式对话 |

### 快速上手示例

如果你想完成最简单的一次完整训练 + 推理，只需三步：

```bash
# 第一步：预训练 — 让模型学会基本的语言能力
python trainer/train_pretrain.py --epochs 1 --batch_size 32

# 第二步：SFT — 让模型学会对话
python trainer/train_full_sft.py --epochs 2 --batch_size 16

# 第三步：推理 — 和你训练的模型聊天！
python eval_llm.py --weight full_sft --device cuda:0
```

就是这么简单。当然，如果你想让模型回答得更好，可以继续进行 DPO、PPO 等对齐训练，
这些我们会在后续章节详细讲解。

---

## 本章小结

让我们回顾一下这一章学到的核心概念：

1. **大语言模型 = 超级文字接龙选手**，它的核心任务是"预测下一个词"
2. **训练 = 大量阅读 + 反复练习预测**，通过海量文本调整模型参数
3. **MiniMind 是一个教学导向的轻量 LLM 框架**，26M~145M 参数，单卡可训练
4. **完整流程：预训练 → SFT → 对齐 → 推理**，每个阶段对应一个独立的训练脚本
5. **9 个训练脚本** 覆盖了当前 LLM 训练的主流方法

在下一章中，我们将深入了解模型如何"认识"文字——分词器（Tokenizer）的工作原理。

[下一章：分词器](02-tokenizer.md)
