import{u as T,r,j as e}from"./index-DB782DBc.js";import{C as E,S as D}from"./SourcePanel-BwVAQ71f.js";import{M as A}from"./constants-BXPCUJh7.js";import{s as I,t as M,m as P}from"./utils-ZSIfeTeC.js";const g=[{name:"Input IDs",shape:"[1, 4]",desc:'输入 token ID 序列，例如 [1, 340, 590, 16] 对应 "<s> 你 好 。"'},{name:"Embedding",shape:"[1, 4, 512]",desc:"Token IDs 通过 Embedding 矩阵查找，每个 ID 映射为 512 维向量。embed_tokens(input_ids)"},{name:"Dropout",shape:"[1, 4, 512]",desc:"训练时随机丢弃部分激活值防止过拟合（推理时关闭）。"},{name:"RoPE 预计算",shape:"cos/sin: [4, 64]",desc:"预计算每个位置的旋转频率 cos(mθ) 和 sin(mθ)，后续所有层共享。"},{name:"Block 0: RMSNorm",shape:"[1, 4, 512]",desc:"第一个 Transformer Block 的输入层归一化：x / RMS(x) × γ，eps=1e-5。"},{name:"Block 0: Attention",shape:"[1, 4, 512]",desc:"GQA 注意力: Q[8头]×K[2头]→scores→softmax→×V[2头]→concat→O_proj。加上残差连接。"},{name:"Block 0: RMSNorm",shape:"[1, 4, 512]",desc:"注意力输出后的第二个 RMSNorm，准备进入 FFN。"},{name:"Block 0: FFN",shape:"[1, 4, 512]",desc:"SwiGLU FFN: down_proj(SiLU(gate_proj(x)) ⊙ up_proj(x))。中间维度 1408。加上残差连接。"},{name:"Block 1-7: ×7",shape:"[1, 4, 512]",desc:"重复相同结构 7 次（共 8 个 Block），每个 Block 包含 Attention + FFN + 2×RMSNorm + 2×残差。"},{name:"Final RMSNorm",shape:"[1, 4, 512]",desc:"最后一个 RMSNorm 归一化，准备投影到词表空间。"},{name:"LM Head",shape:"[1, 4, 6400]",desc:"Linear(512→6400) 投影到词表大小，与 Embedding 共享权重。输出每个位置对 6400 个 token 的 logits。"},{name:"Softmax",shape:"[1, 4, 6400]",desc:"softmax(logits/temperature) 将 logits 转换为概率分布。取最后一个位置的分布用于生成下一个 token。"},{name:"Top-K 采样",shape:"→ token_id",desc:"从概率分布中根据 top-p/top-k 策略采样一个 token，拼接到序列末尾，重复直到遇到 EOS。"}];function K(){const{isDark:d}=T(),[a,u]=r.useState(-1),c=r.useRef(null),[_,b]=r.useState(!1);r.useEffect(()=>()=>{c.current&&clearInterval(c.current)},[]);const y=r.useCallback(()=>{c.current&&(clearInterval(c.current),c.current=null),b(!1)},[]),N=r.useCallback(s=>{u(s)},[]),$=r.useCallback(()=>{u(s=>s<g.length-1?s+1:s)},[]),v=r.useCallback(()=>{u(s=>s>0?s-1:s)},[]),w=r.useCallback(()=>{y(),u(-1)},[y]),B=r.useCallback(()=>{if(c.current){y();return}b(!0),c.current=setInterval(()=>{u(s=>s<g.length-1?s+1:(y(),s))},1200)},[y]),F=r.useMemo(()=>{const s=d?"#e2e8f0":"#1a1a2e",o=d?"#94a3b8":"#555",m=d?"#818cf8":"#4f46e5",p=d?"#34d399":"#10b981",k=d?"#60a5fa":"#3b82f6",i=d?"#fbbf24":"#f59e0b",n=[{y:10,h:30,label:"Input IDs [1,4]",color:o},{y:50,h:30,label:"Embedding [1,4,512]",color:k},{y:90,h:25,label:"Dropout",color:o},{y:122,h:25,label:"RoPE cos/sin",color:i},{y:158,h:25,label:"RMSNorm₁",color:p},{y:192,h:30,label:"Multi-Head Attention (GQA)",color:m},{y:232,h:25,label:"RMSNorm₂",color:p},{y:266,h:30,label:"FFN (SwiGLU)",color:i},{y:308,h:35,label:"× 7 more Blocks",color:o},{y:354,h:25,label:"Final RMSNorm",color:p},{y:390,h:30,label:"LM Head [512→6400]",color:k},{y:430,h:25,label:"Softmax → Probs",color:m},{y:466,h:30,label:"Top-K Sampling → token",color:p}];let l=`<defs><marker id="arrFwd" viewBox="0 0 10 10" refX="9" refY="5" markerWidth="5" markerHeight="5" orient="auto"><path d="M0,0 L10,5 L0,10 Z" fill="${o}"/></marker></defs>`;return n.forEach((t,f)=>{const x=f===a,j=f<a,R=x?.35:j?.2:.08,L=x?2.5:1.5,C=!x&&!j?'stroke-dasharray="4,2"':"";l+=`<rect x="40" y="${t.y}" width="320" height="${t.h}" rx="6" fill="${t.color}" opacity="${R}" stroke="${t.color}" stroke-width="${L}" ${C}/>`,l+=`<text x="200" y="${t.y+t.h/2+5}" text-anchor="middle" fill="${x?s:o}" font-size="${x?12:11}" ${x?'font-weight="bold"':""}>${t.label}</text>`,f<n.length-1&&(l+=`<line x1="200" y1="${t.y+t.h}" x2="200" y2="${n[f+1].y}" stroke="${o}" stroke-width="1" marker-end="url(#arrFwd)" opacity="0.5"/>`),f===5&&(l+=`<path d="M 365 ${n[3].y+n[3].h/2} L 380 ${n[3].y+n[3].h/2} L 380 ${t.y+t.h+5} L 365 ${t.y+t.h+5}" fill="none" stroke="${m}" stroke-width="1" stroke-dasharray="3,2" opacity="0.5"/>`,l+=`<text x="388" y="${(n[3].y+t.y+t.h)/2+5}" fill="${m}" font-size="8" opacity="0.7">+残差</text>`),f===7&&(l+=`<path d="M 365 ${n[5].y+n[5].h+5} L 385 ${n[5].y+n[5].h+5} L 385 ${t.y+t.h+5} L 365 ${t.y+t.h+5}" fill="none" stroke="${i}" stroke-width="1" stroke-dasharray="3,2" opacity="0.5"/>`,l+=`<text x="393" y="${(n[5].y+n[5].h+t.y+t.h)/2+5}" fill="${i}" font-size="8" opacity="0.7">+残差</text>`)}),l+=`<rect x="25" y="155" width="370" height="150" rx="8" fill="none" stroke="${o}" stroke-width="1" stroke-dasharray="6,3" opacity="0.3"/>`,l+=`<text x="30" y="152" fill="${o}" font-size="9" opacity="0.6">TransformerBlock ×${A.num_layers}</text>`,l},[d,a]),S=r.useMemo(()=>{const s=P(777),o=["谢","我","你","很","是","好","啊","的","了","不"],m=o.map(()=>(s()-.3)*5),p=I(m,.8);return o.map((i,n)=>({token:i,prob:p[n]})).sort((i,n)=>n.prob-i.prob)},[]),h=a>=0?g[a]:null;return e.jsxs(e.Fragment,{children:[e.jsx("h2",{children:"7. 推理过程（Forward Pass）"}),e.jsxs("p",{className:"desc",children:["一次完整的推理就是数据在模型中从头到尾走一遍的过程。用代码概括：",e.jsx("code",{children:"logits = lm_head(rms_norm(transformer_blocks(embedding(input_ids))))"}),"， 其中 ",e.jsx("code",{children:"transformer_blocks"})," 重复 8 次，每次包含 Attention + FFN + 残差连接。",e.jsx("br",{}),e.jsxs("small",{style:{color:"var(--fg2)"},children:["关联源码：",e.jsx("code",{children:"model/model_minimind.py:442"})," ",e.jsx("code",{children:"MiniMindForCausalLM.forward()"})," | ",e.jsx("code",{children:":392"})," ",e.jsx("code",{children:"MiniMindModel.forward()"})," | ",e.jsx("code",{children:":365"})," ",e.jsx("code",{children:"MiniMindBlock.forward()"})]})]}),e.jsxs(E,{title:"逐步穿越动画",children:[e.jsx("p",{style:{marginBottom:10,fontSize:"0.9rem",color:"var(--fg2)"},children:'完整的前向传播包含 13 个关键步骤，从输入 token ID 到输出下一个 token 的概率分布。 点击"下一步"逐步执行，观察数据在模型中的流动过程。左侧流程图高亮当前步骤（深色=已完成，虚线=未执行）， 右侧显示该步骤的 tensor shape 变化和详细说明。也可以点击编号圆点直接跳转到任意步骤。 到达最后一步时，会展示模拟的 top-10 预测结果（token 概率分布）。'}),e.jsxs("div",{style:{display:"flex",gap:8,marginBottom:10,flexWrap:"wrap"},children:[e.jsx("button",{className:"btn",onClick:v,children:"◀ 上一步"}),e.jsx("button",{className:"btn primary",onClick:$,children:"下一步 ▶"}),e.jsx("button",{className:"btn",onClick:B,children:_?"⏸ 暂停":"▶ 自动播放"}),e.jsx("button",{className:"btn",onClick:w,children:"重置"})]}),e.jsx("div",{className:"step-indicator",children:g.map((s,o)=>e.jsx("div",{className:`step-dot${o===a?" active":""}${o<a?" done":""}`,onClick:()=>N(o),children:o+1},o))}),e.jsxs("div",{className:"viz-grid",children:[e.jsx("div",{children:e.jsx("svg",{width:"100%",height:520,viewBox:"0 0 400 520",dangerouslySetInnerHTML:{__html:F}})}),e.jsxs("div",{children:[e.jsx("div",{className:"label",children:"当前步骤"}),e.jsx("div",{style:{fontSize:"1.1rem",fontWeight:600,marginBottom:8,color:"var(--accent)"},children:h?h.name:"—"}),e.jsx("div",{className:"label",children:"Tensor Shape"}),e.jsx("div",{className:"shape-badge",style:{marginBottom:12},children:h?h.shape:"—"}),e.jsx("div",{className:"label",children:"说明"}),e.jsx("div",{style:{fontSize:"0.9rem",color:"var(--fg2)",minHeight:80},children:h?h.desc:'点击"下一步"开始'}),e.jsx("div",{className:"label",style:{marginTop:12},children:"Top-10 预测 (最终输出)"}),e.jsx("div",{style:{minHeight:100},children:a===g.length-1?S.map((s,o)=>e.jsxs("div",{style:{display:"flex",alignItems:"center",gap:8,marginBottom:3},children:[e.jsxs("span",{style:{width:30,fontSize:"0.8rem",color:"var(--fg2)",textAlign:"right",fontFamily:"monospace"},children:["#",o+1]}),e.jsx("span",{className:"token-box",style:{background:M(o),color:"#fff",minWidth:28,textAlign:"center"},children:s.token}),e.jsx("div",{style:{flex:1,height:14,background:"var(--bg3)",borderRadius:3,overflow:"hidden"},children:e.jsx("div",{style:{height:"100%",width:s.prob/S[0].prob*100+"%",background:M(o),borderRadius:3,transition:"width 0.5s"}})}),e.jsxs("span",{style:{fontSize:"0.75rem",fontFamily:"monospace",color:"var(--accent)",width:50},children:[(s.prob*100).toFixed(1),"%"]})]},o)):e.jsx("span",{style:{color:"var(--fg3)",fontSize:"0.85rem"},children:"完成前向传播后显示"})})]})]}),e.jsx(D,{title:"对照源码：model/model_minimind.py:392-468",code:`class MiniMindModel(nn.Module):
    """模型主干：Embedding + N × TransformerBlock + RMSNorm"""
    def forward(self, input_ids, ...):
        # Step 1-2: Token ID → Embedding 向量 → Dropout（训练时随机丢弃防止过拟合）
        hidden_states = self.dropout(self.embed_tokens(input_ids))  # [B,S] → [B,S,512]
        # Step 3: 取出预计算的 RoPE cos/sin，所有层共享
        position_embeddings = (freqs_cos[...], freqs_sin[...])
        # Step 4-8: 依次通过 8 个 TransformerBlock
        # 每个 Block: RMSNorm → Attention(+残差) → RMSNorm → FFN(+残差)
        for layer in self.layers:           # 8 × MiniMindBlock
            hidden_states, _ = layer(hidden_states, position_embeddings, ...)
        # Step 9: 最终 RMSNorm 归一化
        hidden_states = self.norm(hidden_states)     # RMSNorm
        return hidden_states  # [B,S,512]

class MiniMindForCausalLM(PreTrainedModel):
    """完整模型：主干 + LM Head（语言模型输出头）"""
    def forward(self, input_ids, labels=None, ...):
        # 通过模型主干获得隐藏状态
        hidden_states, _, aux_loss = self.model(input_ids, ...)
        # Step 10: LM Head 线性投影到词表空间（与 Embedding 共享权重）
        logits = self.lm_head(hidden_states)         # [B,S,512] → [B,S,6400]
        # 训练时：用 logits[:-1] 预测 labels[1:]（next token prediction）
        # labels 中 -100 位置被忽略（PAD 和非训练区域）
        if labels is not None:
            loss = cross_entropy(logits[:-1], labels[1:], ignore_index=-100)
        # 推理时：对 logits 做 softmax → 采样 → 得到下一个 token
        return CausalLMOutputWithPast(loss=loss, logits=logits, ...)`})]})]})}export{K as default};
