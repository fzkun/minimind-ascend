import{u as F,r as x,j as s}from"./index-DB782DBc.js";import{C as S,S as R}from"./SourcePanel-BwVAQ71f.js";import{u as O}from"./useCanvas-BahyMyhc.js";const y=[{name:"Pretrain",x:20,color:"#3b82f6",desc:"预训练：在大规模文本语料上学习语言模式。lr=5e-4, 损失函数=CrossEntropy(next token prediction)。输出: pretrain_512.pth"},{name:"Full SFT",x:170,color:"#10b981",desc:"监督微调：在对话数据上训练，学习指令跟随能力。lr=1e-6, 只对 assistant 回复计算损失。输出: full_sft_512.pth"},{name:"LoRA",x:310,color:"#f59e0b",desc:"LoRA 轻量微调：冻结主干，只训练低秩适配器。rank=16, alpha=16, 可训练参数仅 0.5M。输出: lora_512.pth"},{name:"DPO",x:440,color:"#ef4444",desc:"DPO 对齐：通过 chosen/rejected 对比优化。β=0.1, lr=4e-8。无需训练奖励模型。输出: dpo_512.pth"},{name:"GRPO",x:560,color:"#8b5cf6",desc:"GRPO 在线 RL：Group Relative Policy Optimization，采样多个回复计算组内相对奖励。输出: grpo_512.pth"},{name:"Inference",x:690,color:"#06b6d4",desc:"推理部署：加载训练好的权重，KV Cache + 采样策略 (top-p, temperature) 生成文本。"}],N=[{role:"system",tokens:["<|im_start|>","system","\\n","你","是","Mini","Mind","<|im_end|>"]},{role:"user",tokens:["<|im_start|>","user","\\n","你","好","吗","？","<|im_end|>"]},{role:"assistant",tokens:["<|im_start|>","assistant","\\n","我","很","好","！","<|im_end|>"]}];function I(){const{isDark:n}=F(),[h,D]=x.useState(null),[b,L]=x.useState(.1),w=x.useRef(null),z=x.useMemo(()=>{const e=n?"#e2e8f0":"#1a1a2e",d=n?"#94a3b8":"#555";let o=`<defs><marker id="arrPipe" viewBox="0 0 10 10" refX="9" refY="5" markerWidth="6" markerHeight="6" orient="auto"><path d="M0,0 L10,5 L0,10 Z" fill="${d}"/></marker></defs>`;return y.forEach((l,f)=>{const t=h===f?.4:.15;o+=`<rect x="${l.x}" y="35" width="110" height="42" rx="8" fill="${l.color}" opacity="${t}" stroke="${l.color}" stroke-width="2" style="cursor:pointer" data-stage="${f}"/>`,o+=`<text x="${l.x+55}" y="61" text-anchor="middle" fill="${e}" font-size="12" font-weight="bold" style="pointer-events:none">${l.name}</text>`,f<y.length-1&&(o+=`<line x1="${l.x+110}" y1="56" x2="${y[f+1].x}" y2="56" stroke="${d}" stroke-width="1.5" marker-end="url(#arrPipe)"/>`)}),o+=`<text x="400" y="105" text-anchor="middle" fill="${d}" font-size="10">每个阶段产出 out/{stage}_512.pth 权重文件，下一阶段通过 --from_weight 加载</text>`,o},[n,h]),A=x.useCallback(e=>{const o=e.target.closest("rect[data-stage]");if(o){const l=parseInt(o.getAttribute("data-stage"));D(l)}},[]),C=O((e,d,o)=>{e.fillStyle=n?"#1e293b":"#f8f8f8",e.fillRect(0,0,d,o);const l=50,f=20,t=20,P=30,a=d-l-f,i=o-t-P,v=n?"#e2e8f0":"#1a1a2e",T=n?"#475569":"#ddd";e.strokeStyle=T,e.lineWidth=1,e.beginPath(),e.moveTo(l,t),e.lineTo(l,t+i),e.lineTo(l+a,t+i),e.stroke();const c=[-4,4],j=r=>l+(r-c[0])/(c[1]-c[0])*a,m=n?"#34d399":"#10b981";e.strokeStyle=m,e.lineWidth=2.5,e.beginPath();for(let r=0;r<=a;r++){const _=c[0]+r/a*(c[1]-c[0]),u=1/(1+Math.exp(-b*_)),g=t+i-u*i;r===0?e.moveTo(l+r,g):e.lineTo(l+r,g)}e.stroke();const p=n?"#f87171":"#ef4444";e.strokeStyle=p,e.lineWidth=2,e.beginPath();for(let r=0;r<=a;r++){const _=c[0]+r/a*(c[1]-c[0]),u=1/(1+Math.exp(-b*_)),g=-Math.log(Math.max(u,1e-7)),k=Math.min(g/4,1),M=t+i-k*i;r===0?e.moveTo(l+r,M):e.lineTo(l+r,M)}e.stroke(),e.fillStyle=v,e.font="10px sans-serif",e.textAlign="center",e.fillText("log π_ratio(chosen) - log π_ratio(rejected)",l+a/2,o-3),e.textAlign="right",e.fillText("1.0",l-5,t+5),e.fillText("0",l-5,t+i+3),e.textAlign="left",e.fillStyle=m,e.fillText("σ(β·x)",l+a-60,t+15),e.fillStyle=p,e.fillText("-log σ (loss)",l+a-82,t+28),e.strokeStyle=T,e.lineWidth=.5,e.setLineDash([3,3]),e.beginPath(),e.moveTo(j(0),t),e.lineTo(j(0),t+i),e.stroke(),e.setLineDash([]),e.fillStyle=v,e.font="9px monospace",e.textAlign="center",e.fillText("0",j(0),t+i+12)},[n,b],400,160),$=O((e,d,o)=>{e.fillStyle=n?"#1e293b":"#f8f8f8",e.fillRect(0,0,d,o);const l=60,f=20,t=20,P=35,a=d-l-f,i=o-t-P,v=n?"#e2e8f0":"#1a1a2e",T=n?"#475569":"#ddd";e.strokeStyle=T,e.lineWidth=1,e.beginPath(),e.moveTo(l,t),e.lineTo(l,t+i),e.lineTo(l+a,t+i),e.stroke();const c=(p,r,_)=>_*(.1+.45*(1+Math.cos(Math.PI*p/r))),j=[{lr:5e-4,color:n?"#60a5fa":"#3b82f6"},{lr:1e-6,color:n?"#34d399":"#10b981"},{lr:4e-8,color:n?"#fbbf24":"#f59e0b"}],m=100;j.forEach(p=>{e.strokeStyle=p.color,e.lineWidth=2,e.beginPath();for(let r=0;r<=m;r++){const u=c(r,m,p.lr)/p.lr,g=l+r/m*a,k=t+i-u*i;r===0?e.moveTo(g,k):e.lineTo(g,k)}e.stroke()}),e.fillStyle=v,e.font="10px monospace",e.textAlign="right",e.fillText("lr_max",l-5,t+8),e.fillText("0.1×lr",l-5,t+i+3),e.textAlign="center",e.fillText("0",l,t+i+15),e.fillText("T/2",l+a/2,t+i+15),e.fillText("T",l+a,t+i+15),e.fillText("训练步数 (归一化)",l+a/2,o-3),e.textAlign="left",e.fillText("lr(t) = lr₀ × (0.1 + 0.45 × (1 + cos(πt/T)))",l+10,t+15)},[n],500,200);return s.jsxs(s.Fragment,{children:[s.jsx("h2",{children:"6. 训练流程"}),s.jsxs("p",{className:"desc",children:["MiniMind 的完整训练分多个阶段串联执行，每个阶段产出独立的 ",s.jsx("code",{children:".pth"})," 权重文件，下一阶段通过 ",s.jsx("code",{children:"--from_weight"})," 加载上一阶段的结果继续训练。 流程：Pretrain → SFT → LoRA → DPO → GRPO → Inference。",s.jsx("br",{}),s.jsxs("small",{style:{color:"var(--fg2)"},children:["关联源码：",s.jsx("code",{children:"trainer/train_pretrain.py:23"})," | ",s.jsx("code",{children:"trainer/train_full_sft.py:23"})," | ",s.jsx("code",{children:"trainer/train_dpo.py:33"})," ",s.jsx("code",{children:"dpo_loss()"})," | ",s.jsx("code",{children:"trainer/trainer_utils.py:139"})," ",s.jsx("code",{children:"init_model()"})]})]}),s.jsxs(S,{title:"Pipeline 流程图",children:[s.jsx("p",{style:{marginBottom:10,fontSize:"0.9rem",color:"var(--fg2)"},children:"MiniMind 的训练分为多个阶段，数据从左到右流经各阶段。每个阶段产出独立的 .pth 权重文件， 下一阶段通过 --from_weight 参数加载上一阶段的权重继续训练。点击各阶段方块查看详细说明："}),s.jsx("svg",{ref:w,width:"100%",height:120,viewBox:"0 0 800 120",onClick:A,dangerouslySetInnerHTML:{__html:z}}),s.jsx("div",{style:{marginTop:10,fontSize:"0.85rem",color:"var(--fg2)",minHeight:40},children:h!==null&&s.jsxs("span",{children:[s.jsx("strong",{style:{color:y[h].color},children:y[h].name}),": ",y[h].desc]})})]}),s.jsxs(S,{title:"SFT 损失掩码",children:[s.jsx("p",{style:{marginBottom:10,fontSize:"0.9rem",color:"var(--fg2)"},children:"监督微调（SFT）时，我们只希望模型学习生成 assistant 的回复，而不是复述 system 提示或 user 的问题。 实现方式：将 system/user 部分的 labels 设为 -100（PyTorch CrossEntropyLoss 自动忽略该值）， 只有 assistant 回复处的 labels 保留真实 token ID，从而只对绿色部分计算梯度和损失。"}),s.jsx("div",{style:{padding:10,background:"var(--bg)",borderRadius:"var(--radius)",border:"1px solid var(--border)",overflowX:"auto"},children:s.jsx("div",{style:{display:"flex",flexWrap:"wrap",gap:3,alignItems:"center"},children:N.map((e,d)=>{const o=e.role==="assistant";return e.tokens.map((l,f)=>s.jsx("span",{className:"token-box",style:{background:o?"var(--green)":"var(--bg3)",color:o?"#fff":"var(--fg3)",fontSize:"0.8rem"},title:o?"labels = token_id (计算损失)":"labels = -100 (忽略)",children:l},`${d}-${f}`))})})}),s.jsxs("div",{style:{marginTop:8,display:"flex",gap:16,flexWrap:"wrap",fontSize:"0.85rem"},children:[s.jsxs("span",{children:[s.jsx("span",{className:"token-box",style:{background:"var(--bg3)",color:"var(--fg3)"},children:"灰色"})," = labels=-100（不计算损失）"]}),s.jsxs("span",{children:[s.jsx("span",{className:"token-box",style:{background:"var(--green)",color:"#fff"},children:"绿色"})," = 计算损失的 token"]})]}),s.jsx(R,{title:"对照源码：dataset/lm_dataset.py:74-90 (generate_labels)",code:`def generate_labels(self, input_ids):
    """为 SFT 生成损失掩码：只对 assistant 回复部分计算损失"""
    # 初始化全部为 -100（CrossEntropyLoss 忽略此标签）
    labels = [-100] * len(input_ids)
    i = 0
    while i < len(input_ids):
        # 寻找 assistant 回复的起始标记 <|im_start|>assistant
        if input_ids[i:i+len(self.bos_id)] == self.bos_id:
            start = i + len(self.bos_id)  # 跳过 BOS 标记本身
            end = start
            # 向后扫描直到找到结束标记 <|im_end|>
            while end < len(input_ids):
                if input_ids[end:end+len(self.eos_id)] == self.eos_id:
                    break
                end += 1
            # 将 assistant 回复区间的 labels 设为真实 token ID
            # 这样只有这些位置会产生梯度，system/user 部分不参与训练
            for j in range(start, min(end+len(self.eos_id), self.max_length)):
                labels[j] = input_ids[j]  # 只有 assistant 回复被标记
            i = end + len(self.eos_id)
        else:
            i += 1
    return labels`})]}),s.jsxs(S,{title:"DPO 对比训练",children:[s.jsx("p",{style:{marginBottom:10,fontSize:"0.9rem",color:"var(--fg2)"},children:"DPO（Direct Preference Optimization）是一种无需训练奖励模型的对齐方法。 核心思想：给定同一个问题的好回复（chosen）和差回复（rejected），通过调整模型概率使好回复的似然比差回复更高。 β 参数控制偏离参考模型的惩罚力度——β 越大曲线越陡，模型更快学会区分好坏回复但也更容易过拟合。 拖动滑块观察 σ(β·x) 曲线和 loss 曲线的变化："}),s.jsxs("div",{className:"viz-grid",children:[s.jsxs("div",{children:[s.jsx("div",{className:"label",style:{color:"var(--green)"},children:"✓ Chosen（好回复）"}),s.jsx("div",{style:{padding:8,background:"var(--bg)",border:"2px solid var(--green)",borderRadius:"var(--radius)",fontSize:"0.85rem",minHeight:60},children:"中国的首都是北京，位于华北平原北部。"})]}),s.jsxs("div",{children:[s.jsx("div",{className:"label",style:{color:"var(--red)"},children:"✗ Rejected（差回复）"}),s.jsx("div",{style:{padding:8,background:"var(--bg)",border:"2px solid var(--red)",borderRadius:"var(--radius)",fontSize:"0.85rem",minHeight:60},children:"中国的首都是上海，是最大的城市。"})]})]}),s.jsxs("div",{style:{marginTop:12},children:[s.jsx("div",{className:"label",children:"DPO Loss = -log σ(β × (log π(chosen)/π₀(chosen) - log π(rejected)/π₀(rejected)))"}),s.jsxs("div",{style:{marginTop:6},children:[s.jsx("span",{className:"label",children:"β = "}),s.jsx("input",{type:"range",min:"0.01",max:"0.5",step:"0.01",value:b,onChange:e=>L(parseFloat(e.target.value)),style:{width:200,verticalAlign:"middle"}}),s.jsx("span",{className:"value",children:b.toFixed(2)})]}),s.jsx("canvas",{ref:C,style:{marginTop:8}})]}),s.jsx(R,{title:"对照源码：trainer/train_dpo.py:33-51",code:`def dpo_loss(ref_log_probs, policy_log_probs, mask, beta):
    """DPO 损失函数：通过对比好/坏回复的概率比来优化模型"""
    # 计算序列级平均 log P(seq)：对每个 token 的 log_prob 求和后除以有效长度
    ref_log_probs = (ref_log_probs * mask).sum(dim=1) / seq_lengths
    policy_log_probs = (policy_log_probs * mask).sum(dim=1) / seq_lengths
    # 分别计算策略模型（正在训练的模型）和参考模型（冻结的初始模型）的 log 概率比
    # pi_logratios = log π(chosen) - log π(rejected)，策略模型对好坏回复的偏好差
    pi_logratios = chosen_policy - reject_policy
    # ref_logratios = log π₀(chosen) - log π₀(rejected)，参考模型的偏好差（基线）
    ref_logratios = chosen_ref - reject_ref
    # logits = 策略偏好 - 参考偏好，衡量模型相对于参考的改进量
    logits = pi_logratios - ref_logratios
    # DPO 核心：-log σ(β × logits)
    # 当策略模型比参考更偏好 chosen 时 logits > 0，loss 小
    # 当策略模型反而偏好 rejected 时 logits < 0，loss 大（惩罚）
    # β 控制偏离参考的惩罚强度
    loss = -F.logsigmoid(beta * logits)
    return loss.mean()`})]}),s.jsxs(S,{title:"学习率调度",children:[s.jsx("p",{style:{marginBottom:10,fontSize:"0.9rem",color:"var(--fg2)"},children:"学习率调度对训练稳定性至关重要。MiniMind 使用余弦退火策略：lr 从最大值平滑下降到 0.1×lr_max。 公式为 lr(t) = lr₀ × (0.1 + 0.45 × (1 + cos(π·t/T)))。 不同训练阶段使用差异极大的初始学习率——预训练 5e-4（需要大步探索），SFT 1e-6（微调，小步调整），DPO 4e-8（对齐，极小扰动）。 下图以归一化方式叠加展示三条曲线（形状相同，只是纵轴缩放不同）："}),s.jsx("canvas",{ref:$}),s.jsxs("div",{style:{marginTop:8,display:"flex",gap:16,flexWrap:"wrap",fontSize:"0.85rem"},children:[s.jsx("span",{style:{color:"var(--blue)"},children:"■ Pretrain: 5e-4"}),s.jsx("span",{style:{color:"var(--green)"},children:"■ SFT: 1e-6"}),s.jsx("span",{style:{color:"var(--orange)"},children:"■ DPO: 4e-8"})]}),s.jsx(R,{title:"对照源码：trainer/trainer_utils.py:48-49",code:`def get_lr(current_step, total_steps, lr):
    """余弦退火学习率调度
    - current_step=0 时：lr * (0.1 + 0.45 * (1 + cos(0))) = lr * 1.0 = lr_max
    - current_step=T/2 时：lr * (0.1 + 0.45 * (1 + cos(π/2))) = lr * 0.55
    - current_step=T 时：lr * (0.1 + 0.45 * (1 + cos(π))) = lr * 0.1 = lr_min
    最终学习率不会降到 0，而是保持 0.1 × lr_max 的最小值，避免训练末期完全停滞
    """
    return lr * (0.1 + 0.45 * (1 + math.cos(math.pi * current_step / total_steps)))`})]})]})}export{I as default};
